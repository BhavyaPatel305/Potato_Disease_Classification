{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1064a83-e3ed-40d5-98c5-67cc1786c6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b585fd7-2b63-4843-9319-d1ab19392649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data/images into tensorflow\n",
    "\"\"\"\n",
    "tf.keras.utils.image_dataset_from_directory(\n",
    "    directory,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    "    **kwargs\n",
    ")\n",
    "\"\"\"\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 32 # Usually it's a standard, so 32\n",
    "CHANNELS = 3 # 3 RGB Channels\n",
    "EPOCHS = 50 # It can be any value, but 50 is appropriate\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"AppleVillage\",\n",
    "    shuffle=True,\n",
    "    # All images are 256x256 pixels\n",
    "    image_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3308f011-a03d-4868-9f60-c1e52459890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd6d2aa-80b0-43ea-8ae6-8c03e7f05010",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset) # 501 as we have total 501 batches with each batch of 32 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ed77f-2e32-4121-a6c4-747f453c7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase area for each image being printed to 10x10\n",
    "plt.figure(figsize=(20,20))\n",
    "for image_batch, label_batch in dataset.take(1):\n",
    "    # Display first 12 images of the first batch of 32 images\n",
    "    for i in range(12):\n",
    "        # Make a subplot, like a matrix of images\n",
    "        ax = plt.subplot(3,4,i+1)\n",
    "        \n",
    "        # Everytime we see different image as shuffle=True, so 1st image keeps getting changed\n",
    "        plt.imshow(image_batch[i].numpy().astype(\"uint8\")) # It is float, so convert to int\n",
    "    \n",
    "        # label_batch[0] returns index 0, 1 or 2\n",
    "        # so we do, class_names[label_batch[0]]\n",
    "        # class_names: ['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']\n",
    "        plt.title(class_names[label_batch[i]]) # Which type of image is this, is it early blight, late blight or healthy\n",
    "    \n",
    "        # Remove the axis\n",
    "        plt.axis(\"off\") \n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    print(image_batch[0].numpy()) # Convert Tensor to numpy array, it gives a 3d array of the first image at 0th position where each value is between 0 to 255\n",
    "    print(image_batch[0].shape) # (256, 256, 3)\n",
    "    \n",
    "    print(image_batch.shape) # (32, 256, 256, 3) : 32 images, each image is 256,256, 3 is RGB channels\n",
    "    print(label_batch.numpy()) # Convert tensor to numpy array:\n",
    "    # [1 1 2 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1] : 3 classes so, 0 for early blight, 1 for late blight, 2 for healthy\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077d432-906d-442e-8bdd-5c0c44ff90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spliting DataSet\n",
    "\n",
    "80% ==> training\n",
    "20% ==> 10% validation, 10% testing\n",
    "We run 50 EPOCHS\n",
    "1 epoch means: train the model on the 80% data and then at the end of 1 epoch, validate it using 10% validation data\n",
    "and do this 50 times\n",
    "once this cycle is completed\n",
    "test the model using the 10% testing data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae422f2-a2f2-488c-9643-4740337f3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do all the  data splitting calculations automatically\n",
    "# Parameters:\n",
    "\"\"\"\n",
    "ds: dataset\n",
    "train_split: ratio of training data\n",
    "val_split: ratio of validation data\n",
    "test_split: ration of testing data\n",
    "\"\"\"\n",
    "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    \n",
    "    # Size of dataset\n",
    "    ds_size = len(ds)\n",
    "\n",
    "    # If shuffle is True then shuffle the dataset\n",
    "    if shuffle:\n",
    "            ds = ds.shuffle(shuffle_size, seed=12) # seed is for predictiblity, so if you have same seed everytime, it give you same result everytime, it can be any number (5, 7, ect...)\n",
    "\n",
    "    # Training Data Size\n",
    "    # 80% of dataset(ds) and convert to int, so that we don't get float values\n",
    "    train_size = int(train_split * ds_size)\n",
    "\n",
    "    # Validation Data Size\n",
    "    val_size = int(val_split * ds_size)\n",
    "\n",
    "    # Training Data Set\n",
    "    # Take the first (train_size) batches out of all the batches of data, where each batch has 32 images\n",
    "    train_ds = ds.take(train_size)\n",
    "\n",
    "    # From the remaining 20% of data, take first 10% or (val_size) of data as validation data\n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "\n",
    "    # From the reamining 20% data, skip the first 10% or (val_size) of data and use the remaining data as testing data\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f85325-7e82-4777-85d7-9edab811144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function on the dataset\n",
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c1549-8158-46be-a3fc-19f765b8079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching of the training data set\n",
    "# train_ds.cache(): It will read the image from the memory in the 1st iteration, and when you need the same image, it will keep that image in the memory, so that in every EPOCh we do not need to keep loading it again and again, which improves the performance of the pipeline\n",
    "# shuffle: will shuffle the images\n",
    "# prefetch: If we are using GPU and CPU then, if GPU is busy training the data then prefetch will load the next set of batch from the disk and that improves the performance.\n",
    "\n",
    "# Explaination for : train_ds.cache().shuffle(1000).prefetch()\n",
    "\"\"\"\n",
    " (Why use prefectch?)\n",
    " Refer Image: CPU_GPU_1\n",
    " When loading images, say 32 images at a time and say we have a GPU (eg, Titan RTX)\n",
    " When it(GPU) is training, we are not using CPU when GPU is training because CPU is sitting idle\n",
    " then when GPU is done, CPU again reads the batch and then GPU is added, so for example in image it takes total 12 seconds\n",
    "\n",
    " But when using prefetch and caching : Refer CPU_GPU_2\n",
    " While GPU is training batch 1, CPU will be loading next batch(batch 2), that's what prefetch does basically\n",
    "\n",
    " (Why use cache?)\n",
    " cache on the other hand, refer CACHING_1 image\n",
    " In that, in the first row the blue part represents the images being read\n",
    " So in the first EPOCH we read images, now again in the 2nd EPOCH same images are being read (Which can be seen in the 2nd blue part)\n",
    "\n",
    " But if we use cache(), refere CACHING_2 image\n",
    " In this image, we don't see the blue block again, we see it only once\n",
    " So by doing that we save time reading those images.\n",
    "\"\"\"\n",
    "# prefetch(buffer_size=tf.data.AUTOTUNE) : Let tensorflow determine how many batches to load while GPU is training\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Similarly for validation and testing data set\n",
    "val_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# So far datasets are optimized for training performance, so training will run fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b904fc41-49bf-424d-857b-bc9ecfa319a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a layer to supply to the final model\n",
    "resize_and_rescale = tf.keras.Sequential([ # Layers for pre-processing\n",
    "    # Re-size every image to 256 x 256\n",
    "    #Question: Images are already 256 x 256, why re-size it ?\n",
    "    #Ans: This resize_and_rescale layer will eventiually go to the final model that we are creating and when we\n",
    "    #     have a trained model and when it starts predicting, during prediction if any image that is being supplied to model,\n",
    "    #     which is not 256 x 256, it will automatically re-size it.\n",
    "    tf.keras.layers.Resizing(IMAGE_SIZE, IMAGE_SIZE), \n",
    "    \n",
    "    # It will scale the image to 255\n",
    "    tf.keras.layers.Rescaling(1.0/255),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a532146-2283-41d4-8f49-428b21442fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation to make the model robust\n",
    "\"\"\"\n",
    "# Let's say a model is trained based on certain set of images, now when we upload a new image for which we want the prediction\n",
    "if that image is rotated or is different in contrast then other images in the dataset on which the model is trained\n",
    "then the model will not perform good.\n",
    "for that we need data augentation concept.\n",
    "\n",
    "# What do we do in data augmentation?\n",
    "# Refer DATA_AUGMENTATION image\n",
    "\n",
    "Let' says we have an original image in the training data set, using this same image we create 4 new TRAINING samples by applying different transformations to the original image\n",
    "like horizontal flip, contrast, rotating the image or zooming\n",
    "Now we will use all these 5 images (1 original + 4 new images) for training the model, which makes the model completely robust\n",
    "\n",
    "so now, if this model is used in real world, where a rotated image is being supplied to the model, model knows how to predict that.\n",
    "\"\"\"\n",
    "# Applying random flip and rotation\n",
    "data_augmentation = tf.keras.Sequential([ # Layers for pre-processing\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "# These layers will be used in the actual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5424dc7-d264-4292-b51a-86d9356acb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Shape\n",
    "input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS) # This is the format of the input\n",
    "# Number of classes\n",
    "n_classes = len(class_names)\n",
    "\n",
    "# Building the Convolutional Neural Network (CNN) or Building the model\n",
    "model = models.Sequential([\n",
    "    \n",
    "    # Layer 1: When feed an image, first resize and rescale it\n",
    "    resize_and_rescale,\n",
    "    \n",
    "    # Layer 2: Do the flips and random rotations to generate new samples\n",
    "    data_augmentation, \n",
    "    \n",
    "    # Layer 3: Convolutional Layer\n",
    "    # kernel_size is the size of the filters: Refer image FILTER (In image green box is 3x3 filter), so kernel_size is 3x3\n",
    "    # What's the filter: REFER FILTER_2 image\n",
    "    # In image we have example of Kola's image detection\n",
    "    # 1 filter is to detect eye, another to detect nose, and another to detect ears\n",
    "    # if you have eye, nose, ears of Koala, we can say the image has Koalas head too\n",
    "    # similarly 1 filter to detect hands and legs, we can conclude that image has Koalas body too\n",
    "    # if it has both Koals head and body, means it can be a koalas image, so flattern that and kinda figure that out\n",
    "    # In example, we have 5 layers (eye, nose, ears, hands, legs)\n",
    "\n",
    "    # For this model, we use 32 layers\n",
    "    # Why 32 ?\n",
    "    # Trail and Error, we need a lot of layers where we can detect edges, small features\n",
    "    # Filter_Size: (3,3)\n",
    "    # activation = 'relu' as it is fast to compute\n",
    "    # input_shape: input_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape = input_shape), \n",
    "\n",
    "    # layer 4: Pooling Layer(Like max pooling etc ...): Refer POOLING image\n",
    "    # Benifit: Preserving the features while reducing the size of the image, which is very helpful computation vise.\n",
    "    layers.MaxPooling2D((2,2)), \n",
    "\n",
    "    # Few extra Convolutional and Pooling layers: (After some trail and error I figured I need to stack a few conv and pooling layers)\n",
    "    # 32 and 64 are also figured by trial and error method.\n",
    "    layers.Conv2D(64, kernel_size = (3,3), activation='relu'), \n",
    "    layers.MaxPooling2D((2,2)),\n",
    "\n",
    "    layers.Conv2D(64, kernel_size = (3,3), activation='relu'), \n",
    "    layers.MaxPooling2D((2,2)),\n",
    "\n",
    "    layers.Conv2D(64, (3,3), activation='relu'), \n",
    "    layers.MaxPooling2D((2,2)),\n",
    "\n",
    "    layers.Conv2D(64, kernel_size = (3,3), activation='relu'), \n",
    "    layers.MaxPooling2D((2,2)),\n",
    "\n",
    "    layers.Conv2D(64, kernel_size = (3,3), activation='relu'), \n",
    "    layers.MaxPooling2D((2,2)),\n",
    "\n",
    "    # Next refer FILTER_2 image, now we need to flatten it, so it's an array of neurons, so we have a hidden dense layer and then a final output classification layer\n",
    "    layers.Flatten(),\n",
    "\n",
    "    # Adding a dense layer of 64 neurons\n",
    "    layers.Dense(64, activation='relu'),\n",
    "\n",
    "    # Last layer will have 3 neurons with softmax activation function\n",
    "    # softmax function normalizes the probablity of the classes\n",
    "    layers.Dense(n_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Build the model\n",
    "model.build(input_shape = input_shape)\n",
    "\n",
    "# For Layer 3\n",
    "\"\"\"\n",
    "tf.keras.layers.Conv2D(\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    strides=(1, 1),\n",
    "    padding='valid',\n",
    "    data_format=None,\n",
    "    dilation_rate=(1, 1),\n",
    "    groups=1,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1cfa5-3c64-4fbe-be81-4d676486480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture is ready, still training is not done yet\n",
    "\n",
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c3cb90-beea-45fa-9a97-2d3e18bddf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total params: 183,747 : These are the weights that need to be trained (no.of weights)\n",
    "\n",
    "# So 1st define the neural network architecture, then compile using optimizers like (adam: it is a famous optimizer), then define loss function, metrics\n",
    "# Like in each EPOCH, what kind of metric will you use to track the gradient descent\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e64ce6-a59c-49e4-9203-cbcc96f9574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "# We will record the history of every EPOCH in history parameter to make some plots\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    epochs=EPOCHS, # 50 EPOCHS\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    validation_data=val_ds # During each EPOCH this validation data helps to track the accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da8ad7-3fff-4b60-a6b6-1aec90b4d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running test on test data set\n",
    "scores = model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5237e8-acad-487e-bae5-0ee493caeb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history on matplotlib chart\n",
    "\n",
    "# Store arrays in variables\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffd39d-e998-49f6-9e76-f84ac6645b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for Training Accuracy vs Validation Accuracy\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(EPOCHS), acc, label='Training Accuracy')\n",
    "plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Plot for Training Loss vs Validation Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(EPOCHS), loss, label='Training Loss')\n",
    "plt.plot(range(EPOCHS), val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549589fd-75b3-4127-a078-ac1182e15904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Making Predictions\n",
    "# For now, we take just 1 batch: 32 images\n",
    "for images_batch, labels_batch in test_ds.take(1):\n",
    "\n",
    "    # Take 1st image\n",
    "    # NOTE: Images keeps shuffling, so 1st image will change every time.\n",
    "    first_image = images_batch[0].numpy().astype('uint8')\n",
    "    first_label = labels_batch[0].numpy() # Actual label of the first_image\n",
    "\n",
    "    # Print the image\n",
    "    print(\"first image to predict\")\n",
    "    plt.imshow(first_image)\n",
    "    # Print the actual label of the image\n",
    "    print(\"Actual Label:\", class_names[first_label])\n",
    "\n",
    "    # Prediction\n",
    "    # model.predict expects image batch\n",
    "    # batch_prediction is the prediction for all the 32 images in the batch\n",
    "    batch_prediction = model.predict(image_batch) \n",
    "    # To get prediction of the first image \n",
    "\n",
    "    # output: [9.9377525e-01 6.2247966e-03 1.4793174e-08]\n",
    "    # Our model architecture has 3 neurons in output layer, so 3 values and we have used softmax function\n",
    "    # so these 3 values are 3 probablities and whatever is the highest probablity is our class/answer\n",
    "    # print(batch_prediction[0]) \n",
    "\n",
    "    # Printing predicted label\n",
    "    print(\"Predicted Label:\", class_names[np.argmax(batch_prediction[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d0fbf8-3bb2-471b-94a0-8b20d358490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make the prediction\n",
    "\"\"\"\n",
    "params:\n",
    "model: Takes model as the input\n",
    "img: Takes the image as input for which the prediction is to be made\n",
    "\n",
    "return:\n",
    "predicted_class: Class to which image belongs\n",
    "confidence: With how much confidence does this image belongs to the predicted class/accuracy of prediction\n",
    "\"\"\"\n",
    "def predict(model, img):\n",
    "    # Convert image into image array\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
    "    # Create a batch out of it\n",
    "    img_array = tf.expand_dims(img_array, 0) \n",
    "\n",
    "    # Calling predict function\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    # find the predicted class\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    # find the confidence\n",
    "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f8088-0242-4cfc-885e-6d7c67cbcc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlarge each image being displayed\n",
    "plt.figure(figsize=(20, 20))\n",
    "# Predict for a batch\n",
    "for images, labels in test_ds.take(1):\n",
    "    # Predict for 9 images not the entire batch\n",
    "    for i in range(9):\n",
    "        # Sub-plot to show all 9 images togeather\n",
    "        ax = plt.subplot(3,3,i + 1)\n",
    "        # Show the image\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "\n",
    "        # Predicted Class and Confidence\n",
    "        predicted_class, confidence = predict(model, images[i].numpy())\n",
    "        # Actual Class\n",
    "        actual_class = class_names[labels[i]]\n",
    "\n",
    "        # Print Preducted Class, Confidence, Actual Class as plot title\n",
    "        plt.title(f\"Actual: {actual_class}, \\n Predicted: {predicted_class}. \\n Confidence: {confidence}%\")\n",
    "        \n",
    "        # Remove the axis\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d9265-2691-4e75-a443-022cf4299489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Saving the model\n",
    "model_version = max([int(i) for i in os.listdir(\"../saved_models/apple_models\") + [0]]) + 1\n",
    "folder_path = f\"../saved_models/apple_models/{model_version}\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Save the model inside the folder\n",
    "model.save(os.path.join(folder_path, f\"{model_version}.h5\"))\n",
    "#model.save(f\"../saved_models/{model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a4dc9-15fd-4d36-ac66-7de852802e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
